# -*- coding: utf-8 -*-
"""NLP: WatchReviewAnalysis_KMeans,LDA,HierachicalClustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OsU8S4KwnqCDezW2sMMeJDGbpe7jaLcf

# Document Clustering and Topic Modeling

*In* this project, we use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures.
NLP data preprocessing: First step is to change text to numbers.

## Contents

* [Part 1: Load Data](#Part-1:-Load-Data)
* [Part 2: Tokenizing and Stemming](#Part-2:-Tokenizing-and-Stemming)
* [Part 3: TF-IDF](#Part-3:-TF-IDF)
* [Part 4: K-means clustering](#Part-4:-K-means-clustering)
* [Part 5: Topic Modeling - Latent Dirichlet Allocation](#Part-5:-Topic-Modeling---Latent-Dirichlet-Allocation)

# Part 0: Setup Google Drive Environment
"""

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file = drive.CreateFile({'id':'13gapHq7fada1q72UTGRqX36w8Mi60y3r'}) # id of file you want to access
file.GetContentFile('data.tsv')  # tab-separated

"""# Part 1: Load Data"""

import numpy as np
import pandas as pd
import nltk #nlp常用package
# import gensim

from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

nltk.download('punkt') #nlp常用数据：英文标点符号
nltk.download('stopwords') #没太多意义的词：noise

# Load data into dataframe
df = pd.read_csv('data.tsv', sep='\t', error_bad_lines=False) #error_bad_lines智能读取数据的列, drop lines with too many fields

len(df.index)

"""数据体量：96万"""

df.head()

# Remove missing value
df.dropna(subset=['review_body'],inplace=True) #数据大，直接删掉缺失值

df.reset_index(inplace=True, drop=True)

df.info()

# use the first 1000 data as our training data
data = df.loc[:999, 'review_body'].tolist()

data

"""# Part 2: Tokenizing and Stemming

Tokenize:分词-每个单词都是一个feature，所有的feature组成一个dictionary。一个data set有一个dict，每个data point有自己的matrix。

Stemming：保留词根，比如说去除时态和负数。（一般不使用）

Load stopwords and stemmer function from NLTK library.
Stop words are words like "a", "the", or "in" which don't convey significant meaning.
Stemming is the process of breaking a word down into its root.
"""

# Use nltk's English stopwords.
stopwords = nltk.corpus.stopwords.words('english') #stopwords.append("n't")
#添加无意义的词
stopwords.append("'s")
stopwords.append("'m")
stopwords.append("br") #html <br>
stopwords.append("watch") #data is already watch, so we don't need to take care of watch any more

print ("We use " + str(len(stopwords)) + " stop-words in total.")
print (stopwords[:10])

"""Use our defined functions to analyze (i.e. tokenize, stem) our reviews.

https://www.nltk.org/_modules/nltk/stem/snowball.html
"""

from nltk.stem.snowball import SnowballStemmer
# from nltk.stem import WordNetLemmatizer 

stemmer = SnowballStemmer("english")

# tokenization and stemming
def tokenization_and_stemming(text):
    tokens = []
    # exclude stop words and tokenize the document, generate a list of string 
    for word in nltk.word_tokenize(text):
        if word.lower() not in stopwords:
            tokens.append(word.lower())

    filtered_tokens = []
    
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if token.isalpha():
            filtered_tokens.append(token)
            
    # stemming
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems

tokenization_and_stemming(data[0])

data[0]

"""# Part 3: TF-IDF

TF: Term Frequency，每个词出现的频率。

IDF: Inverse Document Frequency，这个词在本document里面出现的个数/这个词出现过的所有document的数量。

TF-IDF：TF*IDF

wiki:https://zh.wikipedia.org/wiki/Tf-idf
"""

from sklearn.feature_extraction.text import TfidfVectorizer
# define vectorizer parameters
# TfidfVectorizer will help us to create tf-idf matrix
# max_df : maximum document frequency for the given word
# min_df : minimum document frequency for the given word
# max_features: maximum number of words
# use_idf: if not true, we only calculate tf
# stop_words : built-in stop words
# tokenizer: how to tokenize the document
# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram
tfidf_model = TfidfVectorizer(max_df=0.99, #set an upper bound:get rid of words occur 99%times
                              max_features=1000,
                                 min_df=0.01, stop_words='english',
                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,1))

tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses

print ("In total, there are " + str(tfidf_matrix.shape[0]) + \
      " reviews and " + str(tfidf_matrix.shape[1]) + " terms.")

tfidf_matrix

"""1000 data points, 239 features."""

tfidf_matrix.toarray() #todense()

"""matrix to array"""

tfidf_matrix.todense()

"""array to matrix"""

print(type(tfidf_matrix.toarray()))

print(type(tfidf_matrix.todense()))

"""Save the terms identified by TF-IDF."""

# words
tf_selected_words = tfidf_model.get_feature_names()

# print out words
tf_selected_words

"""# Part 4: K-means clustering

kmeans停止条件：k点到组内每个点距离已经不再改变；或者自己设置运行次数（以免运行时间过长）。

找到best k的两种方法：Elbow method，Silhouette method。https://vitalflux.com/elbow-method-silhouette-score-which-better/
"""

# k-means clustering
from sklearn.cluster import KMeans

num_clusters = 5

# number of clusters
km = KMeans(n_clusters=num_clusters, random_state=888)
km.fit(tfidf_matrix)

clusters = km.labels_.tolist()

"""#Elbow method"""

from yellowbrick.cluster import KElbowVisualizer

visualizer = KElbowVisualizer(km, k=(2,10))
visualizer.fit(tfidf_matrix)        # Fit the data to the visualizer
visualizer.show()        # Finalize and render the figure

"""Elbow / SSE Plot: n_clusters = 8 represents the elbow you start seeing diminishing returns by increasing k

#Silhouette method
"""

import matplotlib.pyplot as plt
from yellowbrick.cluster import SilhouetteVisualizer
  
fig, ax = plt.subplots(3, 2, figsize=(15,8))
for i in [5, 6, 7, 8, 9, 10]:
    #Create KMeans instance for different number of clusters
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=888)
    q, mod = divmod(i-3, 2)
    '''
    Create SilhouetteVisualizer instance with KMeans instance
    Fit the visualizer
    '''
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(tfidf_matrix)

""""The major difference between elbow and silhouette scores is that elbow only calculates the euclidean distance whereas silhouette takes into account variables such as variance, skewness, high-low differences, etc. The calculation simplicity of elbow makes it more suited than silhouette score for datasets with smaller size or time complexity. 

Whether all the clusters’ Silhouette plot falls beyond the average Silhouette score. If the silhouette plot for one of the clusters fall below the average Silhouette score, one can reject those numbers of clusters.If there are wider fluctuations, the number of cluster is sub-optimal. "

## 4.1. Analyze K-means Result
"""

# create DataFrame films from all of the input files.
product = { 'review': df[:1000].review_body, 'cluster': clusters}
frame = pd.DataFrame(product, columns = ['review', 'cluster'])

frame.head(10)

print ("Number of reviews included in each cluster:")
frame['cluster'].value_counts().to_frame()

km.cluster_centers_

# 239数的list -> cluster 0的中心点的tf-idf值
#-> assumption: 中心点的值可以代表这个cluster
#-> tf-idf值越大，对应的词越能代表这个document
#-> 选出了tf-idf最大的6个值对应的词来代表这个cluster：weight，最重要的词

km.cluster_centers_.shape

print ("<Document clustering result by K-means>")

#km.cluster_centers_ denotes the importances of each items in centroid.
#We need to sort it in decreasing-order and get the top k items.
order_centroids = km.cluster_centers_.argsort()[:, ::-1] 

Cluster_keywords_summary = {}
for i in range(num_clusters):
    print ("Cluster " + str(i) + " words:", end='')
    Cluster_keywords_summary[i] = []
    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster
        Cluster_keywords_summary[i].append(tf_selected_words[ind])
        print (tf_selected_words[ind] + ",", end='')
    print ()
    
    cluster_reviews = frame[frame.cluster==i].review.tolist()
    print ("Cluster " + str(i) + " reviews (" + str(len(cluster_reviews)) + " reviews): ")
    print (", ".join(cluster_reviews))
    print ()

"""# Part 5: Topic Modeling - Latent Dirichlet Allocation

相对于Kmeans，LDA每个分类数量差不多，Kmeans有些cluster可能特别大。
"""

# Use LDA for clustering
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=5)

# document topic matrix for tfidf_matrix_lda
lda_output = lda.fit_transform(tfidf_matrix)
print(lda_output.shape)
print(lda_output)

# topics and words matrix
topic_word = lda.components_
print(topic_word.shape)
print(topic_word) #correlation

# column names
topic_names = ["Topic" + str(i) for i in range(lda.n_components)]

# index names
doc_names = ["Doc" + str(i) for i in range(len(data))]

df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)

# get dominant topic for each document
topic = np.argmax(df_document_topic.values, axis=1)
df_document_topic['topic'] = topic

df_document_topic.head(10)

df_document_topic['topic'].value_counts().to_frame()

# topic word matrix
print(lda.components_)
# topic-word matrix
df_topic_words = pd.DataFrame(lda.components_)

# column and index
df_topic_words.columns = tfidf_model.get_feature_names()
df_topic_words.index = topic_names

df_topic_words.head()

# print top n keywords for each topic
def print_topic_words(tfidf_model, lda_model, n_words):
    words = np.array(tfidf_model.get_feature_names())
    topic_words = []
    # for each topic, we have words weight
    for topic_words_weights in lda_model.components_:
        top_words = topic_words_weights.argsort()[::-1][:n_words]
        topic_words.append(words.take(top_words))
    return topic_words

topic_keywords = print_topic_words(tfidf_model=tfidf_model, lda_model=lda, n_words=15)        

df_topic_words = pd.DataFrame(topic_keywords)
df_topic_words.columns = ['Word '+str(i) for i in range(df_topic_words.shape[1])]
df_topic_words.index = ['Topic '+str(i) for i in range(df_topic_words.shape[0])]
df_topic_words

"""Advanced:Hierachical clustering.https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/

# Part 6: Hierarchical Clustering
"""

from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(linkage='ward',n_clusters=4).fit(tfidf_matrix.toarray()) #need a dense array, since tfidf_matrix is sparse

hie_clusters = hc.labels_.tolist()

print(hie_clusters)

product = { 'review': df[:1000].review_body, 'cluster': hie_clusters}
frame = pd.DataFrame(product, columns = ['review', 'cluster'])
frame['cluster'].value_counts().to_frame()

"""most data points are clustered into one group(about 8-900), from cluster 2-8"""

import scipy.cluster.hierarchy as shc
plt.figure(figsize=(16, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(tfidf_matrix.toarray(), method='ward'))
plt.axhline(y=4, color='r', linestyle='--')

"""From the dendrogram, we can clearly see that most data have distance less than 5. Threshold 4 gives 11 clusters. When distance is greater than 7, data could be separate into 2 groups. """

new_hc = AgglomerativeClustering(linkage='ward',n_clusters=11).fit(tfidf_matrix.toarray())

new_hc_clusters = new_hc.labels_.tolist()

product = { 'review': df[:1000].review_body, 'cluster': new_hc_clusters}
frame = pd.DataFrame(product, columns = ['review', 'cluster'])
frame['cluster'].value_counts().to_frame()

"""Extension(Other clustering methods):

DBSCAN:密度聚类适用于聚类形状不规则的情况

Spectral Clustering:https://towardsdatascience.com/spectral-clustering-aba2640c0d5b

Mean Shift: https://www.geeksforgeeks.org/ml-mean-shift-clustering/
"""